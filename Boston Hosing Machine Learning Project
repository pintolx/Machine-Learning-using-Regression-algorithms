An end to end machine learning project using regression algorithms
#Loading the libraries
import numpy
from numpy import arange
from matplotlib import pyplot
from pandas import read_csv
from pandas import set_option
from pandas.tools.plotting import scatter_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error

#Loading the data
filename = 'housing.csv'
names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
dataset = read_csv(filename, delim_whitespace=True, names=names) #Attributes are delimited by whitespace 
#Analyzing the data
#We analyze the data by looking at descriptive statistics such as shape, counts, mean, and plots of the data
#Shape of the data
print(dataset.shape) #Has 506 rows and 14 columns

#Datatypes of the data
print(dataset.dtypes) #Mainly made of floats and integers

#Taking a look at the first 20 rows of the dataset
print(dataset.head(20))
#This show that the scales of the different attributes are allover the place because of different measurement units

#Descriptions of each attributes
set_option('precision', 1)
print(dataset.describe()) #counts, mean, standard dev., etc
#Shows that the data needs to be rescaled because the min and max values varry widely

#Ivestigating the correlation of the attributes
set_option('precision', 2)
print(dataset.corr(method='pearson'))

#Some of the attributes have a strong correlation ie above +0.5 or less than -0.5
#Correlations close to or equal to 0 are meaningless
#eg NOX and INDUS have a correlation of 0.77 which makes sense

#Lets take a look at some visualizations of the data
#These help spark some ideas about the data
#Histograms
dataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=2)
pyplot.show()
#Plots show that RAD and TAX have a bimodal distribution(ie 2 distinct peaks), AGE, B, CRIM and NOS have exponential distributions

#Density distributions of the data attributes
dataset.plot(kind='density', sharex=False, sharey=False, subplots=True, layout=(4, 4), fontsize=1, legend=False)
pyplot.show()
#NOX, RM, and LSTAT all have skewed gaussian distributions
#This also confirms the exponential and bimodal distribution suspecions of the other attributes

# box and whisker plots
#These also help us to see the variability in the data
dataset.plot(kind='box', subplots=True, sharex=False, fontsize=8, layout=(4, 4))
pyplot.show()
#So much data looks like outliers

#Multimodial variability of the attributes
#ie how the different attributes relate to one another
#scatter plot matrix
scatter_matrix(dataset)
pyplot.show()

#Visualising the correlation between attributes
fig = pyplot.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')
#cax = ax.matshow(dataset.corr(). vmin=-1, vmax=1, interpolation='none')
fig.colorbar(cax)
ticks = numpy.arange(0,14,1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names)
ax.set_yticklabels(names)
pyplot.show()

#Spliting the dataset into training and validation sets
#We are going to use an 80 - 20 split
array = dataset.values
X = array[:,0:13]
Y = array[:,13]

validation_size = .20
seed = 7
X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)

#Evaluating Algorithms
#We shall use 10 fold cross validation and Mean suared error metric which gives us an idea of how wrong all algorithms tested are
#Test options and evaluation metric
num_fold = 10
scoring = 'neg_mean_squared_error'
#Spot checking algorithms
models = []
names = []
models.append(('LR', LinearRegression()))
models.append(('LASSO', Lasso()))
models.append(('EN', ElasticNet()))
models.append(('KNN', KNeighborsRegressor()))
models.append(('CART', DecisionTreeRegressor()))
models.append(('SVR', SVR()))
#Evaluating each model individually..
results = []
names = []
for name, model in models:
    kfold = KFold(n_splits = num_fold, random_state=seed)
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" %(name, cv_results.mean(), cv_results.std())
    print(msg)
#LR has the lowest MSE, followed by CART
#Lets take a look at the dist of the scores accross all cross validation folds
fig = pyplot.figure()
fig.suptitle("Algorithm Comparison")
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
pyplot.show()

#The plots show that the different scales of the attributes are hurting the skills of the algorithms
#We can solve this by standarding the dataset
#Each attribute is transformed to have a mean values of 0 and standard dev of 1
#During standardization, we use pipelines to prevent data leakages

#Standardizing the data
pipelines = []
pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LinearRegression())])))
pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()), ('LASSO', Lasso())])))
pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()), ('EN', ElasticNet())])))
pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsRegressor())])))
pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsRegressor())])))
pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()), ('SVR', SVR())])))

#Evaluating Algo with transfromed data
results = []
names = []
for name, model in pipelines:
    kfold = KFold(n_splits=num_fold, random_state=seed)
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" %(name, cv_results.mean(), cv_results.std())
    print(msg)
	
#Taking a look at the distributions of the scores accross all validation folds
fig = pyplot.figure()
fig.suptitle('Scaled Algorithm Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
pyplot.show()

#These results show that both KNN and CART have improved scores 
#Exploring KNN further using Algorithm tuning
scaler = StandardScaler().fit(X_train)
rescaledX = scaler.transform(X_train)
k_values = numpy.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21,23, 25])
param_grid = dict(n_neighbors=k_values)
model = KNeighborsRegressor()
kfold = KFold(n_splits=num_fold, random_state=seed)
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring=scoring)
grid_result = grid.fit(rescaledX, Y_train)
#Display the mean, std dev and the best performing values of k
print("Best: %f using %s" %(grid_result.best_score_, grid_result.best_params_))
mean = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(mean, stds, params):
    print("%f (%f) with: %r" %(mean, stdev, param))

#Best performance is at k=3

#Improving performance using Ensemble methods
#We shall use 4 ensemble machine learning algorithms ie 2 Boosting algorithms and 2 Bagging algorithms
#Boosting Methods: AdaBoost and GradientBoostingRegressor
#Bagging methods include Extra Trees and Random Forests
#We shall use 10 fold cross validation and transform the data using the StandardScaler
#Here we go....
ensembles = []
results = []
names = []
ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()), ('AB', AdaBoostRegressor())])))
ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()), ('GBM', GradientBoostingRegressor())])))
ensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()), ('RF', RandomForestRegressor())])))
ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()), ('ET', ExtraTreesRegressor())])))

for name, model in ensembles:
    kfold = KFold(n_splits=num_fold, random_state=seed)
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" %(name, cv_results.mean(), cv_results.std())
    print(msg)

#Comparing the algorithms using box plots
fig = pyplot.figure()
fig.suptitle('Ensemble Methods Comparisons')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
pyplot.show()

#Tuning Gradient Boosting Ensemble to improve performance
scaler = StandardScaler().fit(X_train)
rescaledX = scaler.transform(X_train)
boostStages = numpy.array([50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600])
param_grid = dict(n_estimators=boostStages)
model = GradientBoostingRegressor(random_state=seed)
kfold = KFold(n_splits=num_fold, random_state=seed)
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring=scoring)
grid_result = grid.fit(rescaledX, Y_train)
#vIEWING THE RESULTS
print("Best: %f using %s" %(grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with %r" %(mean, stdev, param))
	

#Finalizing the model
#Preparing the model and running it over all the training dataset
scaler = StandardScaler().fit(X_train)
rescaledX = scaler.transform(X_train)
model  = GradientBoostingRegressor(random_state=seed, n_estimators=600 )
model.fit(rescaledX, Y_train)

#Rescaling the validation dataset
rescaledValidationX = scaler.transform(X_validation)
predictions = model.predict(rescaledValidationX)
print(mean_squared_error(Y_validation, predictions))
